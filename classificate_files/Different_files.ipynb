{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7a6ee66-05ec-4d9c-a8ae-222ee5eaf75b",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=uM4u7P2xkO8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58743440-10f3-441c-acfe-c31075569f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.0-rc0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pathlib\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7ebba4-fb38-4048-8a20-96f0310c851e",
   "metadata": {},
   "source": [
    "Utilizar los documentos .txt\n",
    "- Leer los .txt en orden: papel, piedra, tijera\n",
    "- Meter 700 docs para el entreno\n",
    "- Y los otros 300 para la prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f114daeb-4cf4-4b07-9af5-b1ffa6607088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrays vacíos creados correctamente.\n",
      "Array de ENTRENAMIENTO completado con los datos correctamente.\n"
     ]
    }
   ],
   "source": [
    "clases = {\"papel\" : 0, \"piedra\" : 1, \"tijera\" : 2}\n",
    "num_entrena = 35000\n",
    "num_prueba = 15000\n",
    "word_length = 20\n",
    "palabras_entrena = np.zeros((num_entrena * len(clases), 1, word_length))\n",
    "clases_entrena = np.zeros((num_entrena * len(clases)))\n",
    "palabras_prueba = np.zeros((num_prueba * len(clases), 1, word_length))\n",
    "clases_prueba = np.zeros((num_prueba * len(clases)))\n",
    "\n",
    "print(\"Arrays vacíos creados correctamente.\")\n",
    "\n",
    "for i in range(num_entrena):\n",
    "    for clase in clases:\n",
    "        palabra = open(\"dataset_rand\\\\\" + clase + \"\\\\\" + str(i) + \".txt\", \"r\").read()\n",
    "        indice = i + clases[clase] * num_entrena\n",
    "        for j in range(word_length):\n",
    "            palabras_entrena[indice][0][j] = int.from_bytes(list(palabra)[j].encode(), byteorder=\"big\")\n",
    "            clases_entrena[indice] = clases[clase]\n",
    "\n",
    "print(\"Array de ENTRENAMIENTO completado con los datos correctamente.\")\n",
    "\n",
    "for i in range(num_entrena, num_prueba + num_entrena):\n",
    "    for clase in clases:\n",
    "        palabra = open(\"dataset_rand\\\\\" + clase + \"\\\\\" + str(i) + \".txt\", \"r\").read()\n",
    "        indice = i + clases[clase] * num_prueba - num_entrena\n",
    "        for j in range(word_length):\n",
    "            palabras_prueba[indice][0][j] = int.from_bytes(list(palabra)[j].encode(), byteorder=\"big\")\n",
    "            clases_prueba[indice] = clases[clase]\n",
    "\n",
    "print(\"Array de PRUEBA completado con los datos correctamente.\")\n",
    "print(\"Proceso completado\")\n",
    "            \n",
    "# print(palabras_entrena)\n",
    "# print(clases_entrena)\n",
    "# print(\"------------------------------------\")\n",
    "# print(palabras_prueba)\n",
    "# print(clases_prueba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e8a5b0-35d5-446f-853c-1aa6dd7289e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clases = {\"papel\" : 0, \"piedra\" : 1, \"tijera\" : 2}\n",
    "palabras_entrena = [\"sjopgdjtkpapelkamfbr\", \"qolsjdeltpiedrawlpsr\", \"wlfncuwptijeraqlmsyp\"]\n",
    "docs_entrena = np.zeros((3, 1, 20))\n",
    "clases_entrena = np.zeros((3))\n",
    "\n",
    "for i in range(len(palabras_entrena)):\n",
    "    for j in range(len(palabras_entrena[i])):\n",
    "        docs_entrena[i][0][j] = int.from_bytes(list(palabras_entrena[i])[j].encode(), byteorder=\"big\")\n",
    "        clases_entrena[i] = i\n",
    "print(\"docs_entrena\")\n",
    "print(docs_entrena)\n",
    "print(\"clases_entrena\")\n",
    "print(clases_entrena)\n",
    "\n",
    "palabras_prueba = [\"holaadiospapelkslpwq\", \"piedraqjcmalpwjdvmol\", \"qpalskmcetupfhtijera\"]\n",
    "docs_prueba = np.zeros((3, 1, 20))\n",
    "clases_prueba = np.zeros((3))\n",
    "\n",
    "for i in range(len(palabras_prueba)):\n",
    "    for j in range(len(palabras_prueba[i])):\n",
    "        docs_prueba[i][0][j] = int.from_bytes(list(palabras_prueba[i])[j].encode(), byteorder=\"big\")\n",
    "        clases_prueba[i] = i\n",
    "print(\"docs_prueba\")\n",
    "print(docs_prueba)\n",
    "print(\"clases_prueba\")\n",
    "print(clases_prueba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4d2006-32b8-4572-97d4-b978a5ea5e54",
   "metadata": {},
   "source": [
    "Escalar imagenes de [0,255] a [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d696ed-84f1-4252-a5c5-443fd57ba724",
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_entrena = palabras_entrena / 126\n",
    "palabras_prueba = palabras_prueba / 126"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d580a01f-85e8-4126-b889-fbfa801a6b9c",
   "metadata": {},
   "source": [
    "Crear la estructura del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e5e47f-b19a-4c37-8878-dc48ece3bc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(1,20)), # hace cambios en el formato de la matriz\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"), # 64 neuronas, funcion de activación relu\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(3), # 3 neuronas porque hay 3 clases\n",
    "    tf.keras.layers.Softmax()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b501844-9b3c-49f2-8609-a20a40aa4db5",
   "metadata": {},
   "source": [
    "Compilar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50224b9-8684-4dff-8283-69d8fdfae289",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.compile(optimizer=\"sgd\",\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641c7da9-f9b2-4d4f-b1f9-1ad2bcd6e17e",
   "metadata": {},
   "source": [
    "Entrenar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd53a8d-1460-4c4c-8c1f-e785459ac794",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "historial = modelo.fit(palabras_entrena, clases_entrena, epochs=100) #700"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec73a62f-3750-463e-bcc0-3736c2d2cfdf",
   "metadata": {},
   "source": [
    "Predecir imágenes de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3e70f1-7003-4172-ae2c-e7c5cecbc676",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = modelo.evaluate(palabras_entrena, clases_entrena, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294b8d49-3240-4bf4-b1a5-6f0da5babad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Magnitud de pérdida\")\n",
    "plt.plot(historial.history[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b644e6ed-3fe7-4dcd-a566-a92dd54182be",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicciones = modelo.predict(palabras_prueba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2814792c-eca0-4d5a-b66d-e229ca20d852",
   "metadata": {},
   "outputs": [],
   "source": [
    "for instancia in range(palabras_prueba.shape[0]):\n",
    "    print(predicciones[instancia])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dc4ee6-aaff-40c5-9757-408cb2a937cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de arreglo para almacenar predicciones\n",
    "clase_predicha = np.empty(palabras_prueba.shape[0], dtype = \"uint8\")\n",
    "\n",
    "for instancia in range(palabras_prueba.shape[0]):\n",
    "  # almacena clase predicha para una imagen dada\n",
    "  clase_predicha[instancia] = np.argmax(predicciones[instancia])\n",
    "  #if clase_predicha[instancia] == clases_prueba[instancia]:\n",
    "    #print(\"Probabilidades:\", predicciones[instancia],\n",
    "          #\"Clase predicha:\", clase_predicha[instancia],\n",
    "          #\"Clase correcta:\", clases_prueba[instancia],\n",
    "          #\"La Red Neuronal ACERTÓ\")\n",
    "  #else:\n",
    "    #print(\"Probabilidades:\", predicciones[instancia],\n",
    "          #\"Clase predicha:\", clase_predicha[instancia],\n",
    "          #\"Clase correcta:\", clases_prueba[instancia],\n",
    "          #\"La Red Neuronal ERRÓ\")\n",
    "print(clase_predicha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5161bb3-fabe-4036-bb12-b1118e1d2b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz = tf.math.confusion_matrix(clases_prueba, clase_predicha)\n",
    "print(\"Matriz de Confusión:\\n\", matriz.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
